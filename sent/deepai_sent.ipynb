{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e50957",
   "metadata": {
    "height": 81
   },
   "outputs": [],
   "source": [
    "%pip install bardapi\n",
    "%pip install openai\n",
    "%pip install -q -U google-generativeai\n",
    "%pip install hugchat\n",
    "%pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "139ddf55",
   "metadata": {
    "height": 183
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import ast\n",
    "# from google.colab import userdata\n",
    "# import google.generativeai as genai\n",
    "# from hugchat import hugchat\n",
    "# from hugchat.login import Login\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21041f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "openai.api_key  = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "33dadd5c",
   "metadata": {
    "height": 529
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read from original Dataset\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "# if path.exists('./drive/MyDrive/SemEval2024/Task9/SP_new_test.npy')\n",
    "# ./drive/MyDrive/SemEval2024/Task9/Round_{round}.csv\n",
    "folder_path = '/home/jovyan/work'\n",
    "pattern = 'Round_*.csv'\n",
    "\n",
    "# Find all files matching the pattern\n",
    "files = glob.glob(os.path.join(folder_path, pattern))\n",
    "\n",
    "if files:\n",
    "    # Extract round numbers from the file names\n",
    "    rounds = [int(file.split('_')[1].split('.')[0]) for file in files]\n",
    "\n",
    "    # Find the file with the highest round\n",
    "    max_round_file = max(zip(rounds, files), key=lambda x: x[0])[1]\n",
    "\n",
    "    # Read the DataFrame from the file with the highest round\n",
    "    df = pd.read_csv(max_round_file)\n",
    "\n",
    "    # Display the DataFrame or perform other operations\n",
    "    # print(df)\n",
    "    print(\"Read from existing DataFrame\")\n",
    "else:\n",
    "    SP_eval_data = np.load('/home/jovyan/work/SP_new_test.npy', allow_pickle=True)\n",
    "    df = pd.DataFrame(list(SP_eval_data))\n",
    "    print(\"Read from original Dataset\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e81b0fae",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "df_columns = df.columns\n",
    "df_columns\n",
    "num_rounds = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d05fb8",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "model_names = ['ChatGPT', 'Gemini', 'Mistral']\n",
    "\n",
    "import openai\n",
    "\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, # this is the degree of randomness of the model's output\n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]\n",
    "\n",
    "def get_ChatGPT_answer(row, round):\n",
    "    choice_list = None\n",
    "    if isinstance(row['choice_list'], str):\n",
    "        choice_list = ast.literal_eval(row['choice_list'])\n",
    "    elif isinstance(row['choice_list'], list):\n",
    "        choice_list = row['choice_list']\n",
    "    prompt = f\"\"\"\"\"\"\n",
    "    if round == 1:\n",
    "#         prompt += f\"\"\"\n",
    "# you are facing with a riddle or tricky sentence. let's work this out in a step by step way to be sure we have the right answer. find a way to solve it based on options and tell your reason. please before answering the question see options want what? :\n",
    "# Q: {row['question']}\n",
    "# 0) {choice_list[0]}\n",
    "# 1) {choice_list[1]}\n",
    "# 2) {choice_list[2]}\n",
    "# 3) {choice_list[3]}\n",
    "\n",
    "# write your answer in the following format:\n",
    "# Answer: <option_number(0-based)>\n",
    "# Confidence: <confidence of you answer from 0 to 100>%\n",
    "# Reason: <reason>\n",
    "# \"\"\"\n",
    "        prompt += f\"\"\"\n",
    "let's think step by step. among all possible answer choose best one.\n",
    "Riddle: {row['question']}\n",
    "option 0) {choice_list[0]}\n",
    "option 1) {choice_list[1]}\n",
    "option 2) {choice_list[2]}\n",
    "option 3) {choice_list[3]}\n",
    "\n",
    "write your answer in the following format:\n",
    "Answer: <option_number(0-based)>\n",
    "Confidence: <confidence of you answer from 0 to 100>%\n",
    "Reason: <reason>\n",
    "\"\"\"\n",
    "    else:\n",
    "#         prompt += f\"\"\"\n",
    "# you are facing with a riddle or tricky sentence. let's work this out in a step by step way to be sure we have the right answer. find a way to solve it based on options and tell your reason. please before answering the question see options want what? :\n",
    "# Q: {row['question']}\n",
    "# 0) {choice_list[0]}\n",
    "# 1) {choice_list[1]}\n",
    "# 2) {choice_list[2]}\n",
    "# 3) {choice_list[3]}\n",
    "\n",
    "# these are answers of other agents (their answer is also in 0-based format):\n",
    "# \"\"\"\n",
    "        prompt += f\"\"\"\n",
    "let's think step by step. among all possible answer choose best one.\n",
    "Riddle: {row['question']}\n",
    "option 0) {choice_list[0]}\n",
    "option 1) {choice_list[1]}\n",
    "option 2) {choice_list[2]}\n",
    "option 3) {choice_list[3]}\n",
    "\n",
    "these are answers of other agents (their answer is also in 0-based format):\n",
    "\"\"\"\n",
    "        tmp = [f\"some agent answer: {row[f'Round_{round-1}_Model_{model_name}']}\" for idx, model_name in enumerate(model_names)]\n",
    "        s = '\\n'.join(tmp)\n",
    "        prompt += s\n",
    "\n",
    "        prompt += \"\"\"\n",
    "Check if the answer of other agents are logical or not and if you think they don't have logical reason, write your logical reason. Write your answer to the tricky question in the following format:\n",
    "Answer: <option_number (0-based)>\n",
    "Confidence: <confidence of you answer from 0 to 100>%\n",
    "Reason: <reason>\n",
    "\"\"\"\n",
    "    print(prompt)\n",
    "    print(50*\"*\")\n",
    "    response = get_completion(prompt)\n",
    "    return response\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_gemini_answer(row, round):\n",
    "    raise Exception(\"exception\")\n",
    "    GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
    "    genai.configure(api_key=GOOGLE_API_KEY)\n",
    "    model = genai.GenerativeModel('gemini-pro')\n",
    "    choice_list = None\n",
    "    if isinstance(row['choice_list'], str):\n",
    "        choice_list = ast.literal_eval(row['choice_list'])\n",
    "    elif isinstance(row['choice_list'], list):\n",
    "        choice_list = row['choice_list']\n",
    "\n",
    "    prompt = f\"\"\"\"\"\"\n",
    "    if round == 1:\n",
    "        prompt += f\"\"\"\n",
    "let's think step by step. among all possible answer choose best one.\n",
    "Riddle: {row['question']}\n",
    "option 0) {choice_list[0]}\n",
    "option 1) {choice_list[1]}\n",
    "option 2) {choice_list[2]}\n",
    "option 3) {choice_list[3]}\n",
    "\n",
    "write your answer in the following format:\n",
    "Answer: <option_number(0-based)>\n",
    "Confidence: <confidence of you answer from 0 to 100>%\n",
    "Reason: <reason>\n",
    "\"\"\"\n",
    "    else:\n",
    "        prompt += f\"\"\"\n",
    "let's think step by step. among all possible answer choose best one.\n",
    "Riddle: {row['question']}\n",
    "option 0) {choice_list[0]}\n",
    "option 1) {choice_list[1]}\n",
    "option 2) {choice_list[2]}\n",
    "option 3) {choice_list[3]}\n",
    "\n",
    "these are answers of other agents (their answer is also in 0-based format):\n",
    "\"\"\"\n",
    "        tmp = [f\"some agent answer: {row[f'Round_{round-1}_Model_{model_name}']}\" for idx, model_name in enumerate(model_names)]\n",
    "        s = '\\n'.join(tmp)\n",
    "        prompt += s\n",
    "\n",
    "        prompt += \"\"\"\n",
    "Check if the answer of other agents are logical or not and if you think they don't have logical reason, write your logical reason. Write your answer to the tricky question in the following format:\n",
    "Answer: <option_number (0-based)>\n",
    "Confidence: <confidence of you answer from 0 to 100>%\n",
    "Reason: <reason>\n",
    "\"\"\"\n",
    "    response = model.generate_content(prompt)\n",
    "\n",
    "    return response.text\n",
    "\n",
    "\n",
    "\n",
    "def get_Mistral_answer(row, round):\n",
    "    LOGIN_EMAIL=userdata.get('LOGIN_EMAIL')\n",
    "    LOGIN_PASS=userdata.get('LOGIN_PASS')\n",
    "    # sign = Login(email=\"\", passwd=\"\")\n",
    "    sign = Login(email=LOGIN_EMAIL, passwd=LOGIN_PASS)\n",
    "    cookies = sign.login()\n",
    "\n",
    "    # Save cookies to the local directory\n",
    "    cookie_path_dir = \"./cookies_snapshot\"\n",
    "    sign.saveCookiesToDir(cookie_path_dir)\n",
    "\n",
    "    # Create a ChatBot\n",
    "    chatbot = hugchat.ChatBot(cookies=cookies.get_dict())  # or cookie_path=\"usercookies/<email>.json\"\n",
    "\n",
    "    # Create a new conversation\n",
    "    id = chatbot.new_conversation()\n",
    "    chatbot.change_conversation(id)\n",
    "\n",
    "\n",
    "\n",
    "    choice_list = None\n",
    "    if isinstance(row['choice_list'], str):\n",
    "        choice_list = ast.literal_eval(row['choice_list'])\n",
    "    elif isinstance(row['choice_list'], list):\n",
    "        choice_list = row['choice_list']\n",
    "\n",
    "    prompt = f\"\"\"\"\"\"\n",
    "    if round == 1:\n",
    "        prompt += f\"\"\"\n",
    "let's think step by step. among all possible answer choose best one.\n",
    "Riddle: {row['question']}\n",
    "option 0) {choice_list[0]}\n",
    "option 1) {choice_list[1]}\n",
    "option 2) {choice_list[2]}\n",
    "option 3) {choice_list[3]}\n",
    "\n",
    "write your answer in the following format:\n",
    "Answer: <option_number(0-based)>\n",
    "Confidence: <confidence of you answer from 0 to 100>%\n",
    "Reason: <reason>\n",
    "\"\"\"\n",
    "    else:\n",
    "        prompt += f\"\"\"\n",
    "let's think step by step. among all possible answer choose best one.\n",
    "Riddle: {row['question']}\n",
    "option 0) {choice_list[0]}\n",
    "option 1) {choice_list[1]}\n",
    "option 2) {choice_list[2]}\n",
    "option 3) {choice_list[3]}\n",
    "\n",
    "these are answers of other agents (their answer is also in 0-based format):\n",
    "\"\"\"\n",
    "        tmp = [f\"some agent answer: {row[f'Round_{round-1}_Model_{model_name}']}\" for idx, model_name in enumerate(model_names)]\n",
    "        s = '\\n'.join(tmp)\n",
    "        prompt += s\n",
    "\n",
    "        prompt += \"\"\"\n",
    "Check if the answer of other agents are logical or not and if you think they don't have logical reason, write your logical reason. Write your answer to the tricky question in the following format:\n",
    "Answer: <option_number (0-based)>\n",
    "Confidence: <confidence of you answer from 0 to 100>%\n",
    "Reason: <reason>\n",
    "\"\"\"\n",
    "\n",
    "    # non stream response\n",
    "    response = chatbot.query(prompt)['text']\n",
    "    # print(query_result) # or query_result.text or query_result[\"text\"]\n",
    "    return response\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "get_model_answer = [get_ChatGPT_answer, get_gemini_answer, get_Mistral_answer]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c8439f",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "import re\n",
    "round_models = {it:[] for it in model_names}\n",
    "for model in model_names:\n",
    "    for col in df_columns:\n",
    "        if col.startswith('Round_') and f'_Model_{model}' in col:\n",
    "            parts = col.split('_')\n",
    "            round_num = int(parts[1])\n",
    "            model_name = parts[3]\n",
    "            round_models[model_name] = sorted(round_models.get(model_name, []) + [round_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7ac908",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "round_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021d1a0b",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "saved = []\n",
    "for k in round_models.keys():\n",
    "    if len(round_models[k]) > 0:\n",
    "        if df[f'Round_{max(round_models[k])}_Model_{k}'].notnull().all():\n",
    "            saved.append(max(round_models[k]))\n",
    "        else:\n",
    "            saved.append(max(round_models[k])-1)\n",
    "    else:\n",
    "        saved.append(0)\n",
    "\n",
    "last_saved_round = min(saved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683f6631",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "first_unsaved_round = last_saved_round + 1\n",
    "first_unsaved_round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f097f3",
   "metadata": {
    "height": 30
   },
   "outputs": [],
   "source": [
    "def get_value(model_name, row, current_round):\n",
    "    response = get_model_answer[model_names.index(model_name)](row, current_round)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9679b55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "for current_round in range(first_unsaved_round, num_rounds + 1):\n",
    "    for model_name in model_names:\n",
    "        if f'Round_{current_round}_Model_{model_name}' not in df.columns:\n",
    "            df[f'Round_{current_round}_Model_{model_name}'] = pd.Series()\n",
    "        for index, row in tqdm(df.iterrows(), total = len(df), desc = f\"{model_name}\"):\n",
    "            if pd.isnull(row[f'Round_{current_round}_Model_{model_name}']):\n",
    "                model_response = None\n",
    "                while True:\n",
    "                    try:\n",
    "                        model_response = get_value(model_name, row, current_round)\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        print(f\"An error happened ({e}). I repeat it.\")\n",
    "                # print(model_response)\n",
    "                df.at[index, f'Round_{current_round}_Model_{model_name}'] = model_response\n",
    "                df.to_csv(f'/home/jovyan/work/Round_{current_round}.csv', index=False)  # Save DataFrame to CSV after each row"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
